{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCDAA Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LQRController:\n",
    "    def __init__(self, H, M, D, C, R, T):\n",
    "        \"\"\"\n",
    "        Initialize the LQRController class with problem parameters.\n",
    "\n",
    "        \"\"\"\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.D = D\n",
    "        self.C = C\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "    def solve_riccati_ode(self, time_grid):\n",
    "        \n",
    "        def riccati_ode(t, S_flat):\n",
    "            S = torch.tensor(S_flat).reshape((2, 2))  # Convert S_flat to a PyTorch tensor\n",
    "            dS = -2 * torch.matmul(self.H.t(), S) + torch.matmul(S, torch.matmul(self.M, torch.matmul(torch.inverse(self.D), self.M.t()))) - self.C\n",
    "            return dS.flatten().detach().numpy()\n",
    "\n",
    "        S_final_flat = self.R.flatten().detach().numpy()\n",
    "        sol = solve_ivp(riccati_ode, [self.T.item(), time_grid[0].item()], S_final_flat, t_eval=time_grid.numpy()[::-1], method='RK45')\n",
    "        S_values = [torch.tensor(sol.y[:, i].reshape((2, 2))) for i in range(len(sol.t))]\n",
    "        return S_values\n",
    "\n",
    "    def optimal_control_value(self, time_grid, x_values):\n",
    "        \n",
    "        S_values = self.solve_riccati_ode(time_grid)\n",
    "        control_values = []\n",
    "        for S in S_values:\n",
    "            S_tensor = torch.tensor(S)  # Convert S to a PyTorch tensor\n",
    "            control = -torch.matmul(torch.inverse(self.D), torch.matmul(self.M.t(), torch.matmul(S_tensor, state_values.view(-1, 2).t()))).detach().unsqueeze(1)\n",
    "            control_values.append(control)\n",
    "        control_values = torch.cat(control_values, dim=1)\n",
    "        return control_values\n",
    "\n",
    "    def optimal_control_function(self, time_grid, x_values):\n",
    "       \n",
    "        S_values = self.solve_riccati_ode(time_grid)\n",
    "        control_functions = []\n",
    "        for S in S_values:\n",
    "            S_tensor = torch.tensor(S)  # Convert S to a PyTorch tensor\n",
    "            control = -torch.matmul(torch.inverse(self.D), torch.matmul(self.M.t(), torch.matmul(S_tensor, state_values.view(-1, 2).t()))).detach().t()\n",
    "            control_functions.append(control)\n",
    "        control_functions = torch.stack(control_functions)\n",
    "        return control_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Problem Value:\n",
      "tensor([[[-0.5000, -1.0000],\n",
      "         [-0.5102, -1.0203],\n",
      "         [-0.5204, -1.0408],\n",
      "         [-0.5308, -1.0615],\n",
      "         [-0.5412, -1.0825],\n",
      "         [-0.5518, -1.1036],\n",
      "         [-0.5625, -1.1250],\n",
      "         [-0.5733, -1.1465],\n",
      "         [-0.5842, -1.1683],\n",
      "         [-0.5952, -1.1903],\n",
      "         [-0.6063, -1.2126],\n",
      "         [-0.6175, -1.2350],\n",
      "         [-0.6289, -1.2577],\n",
      "         [-0.6403, -1.2807],\n",
      "         [-0.6519, -1.3038],\n",
      "         [-0.6636, -1.3272],\n",
      "         [-0.6754, -1.3508],\n",
      "         [-0.6874, -1.3747],\n",
      "         [-0.6994, -1.3988],\n",
      "         [-0.7116, -1.4232],\n",
      "         [-0.7239, -1.4478],\n",
      "         [-0.7363, -1.4727],\n",
      "         [-0.7489, -1.4978],\n",
      "         [-0.7616, -1.5231],\n",
      "         [-0.7744, -1.5488],\n",
      "         [-0.7873, -1.5746],\n",
      "         [-0.8004, -1.6008],\n",
      "         [-0.8136, -1.6272],\n",
      "         [-0.8269, -1.6539],\n",
      "         [-0.8404, -1.6808],\n",
      "         [-0.8540, -1.7080],\n",
      "         [-0.8678, -1.7355],\n",
      "         [-0.8816, -1.7633],\n",
      "         [-0.8957, -1.7913],\n",
      "         [-0.9098, -1.8197],\n",
      "         [-0.9241, -1.8483],\n",
      "         [-0.9386, -1.8772],\n",
      "         [-0.9532, -1.9064],\n",
      "         [-0.9680, -1.9359],\n",
      "         [-0.9829, -1.9657],\n",
      "         [-0.9979, -1.9958],\n",
      "         [-1.0131, -2.0262],\n",
      "         [-1.0285, -2.0569],\n",
      "         [-1.0440, -2.0880],\n",
      "         [-1.0596, -2.1193],\n",
      "         [-1.0755, -2.1510],\n",
      "         [-1.0915, -2.1829],\n",
      "         [-1.1076, -2.2152],\n",
      "         [-1.1239, -2.2479],\n",
      "         [-1.1404, -2.2808],\n",
      "         [-1.1571, -2.3141],\n",
      "         [-1.1739, -2.3478],\n",
      "         [-1.1909, -2.3817],\n",
      "         [-1.2080, -2.4161],\n",
      "         [-1.2254, -2.4507],\n",
      "         [-1.2429, -2.4857],\n",
      "         [-1.2606, -2.5211],\n",
      "         [-1.2784, -2.5569],\n",
      "         [-1.2965, -2.5930],\n",
      "         [-1.3147, -2.6294],\n",
      "         [-1.3331, -2.6663],\n",
      "         [-1.3517, -2.7035],\n",
      "         [-1.3705, -2.7411],\n",
      "         [-1.3895, -2.7791],\n",
      "         [-1.4087, -2.8174],\n",
      "         [-1.4281, -2.8562],\n",
      "         [-1.4477, -2.8953],\n",
      "         [-1.4674, -2.9349],\n",
      "         [-1.4874, -2.9748],\n",
      "         [-1.5076, -3.0152],\n",
      "         [-1.5280, -3.0559],\n",
      "         [-1.5485, -3.0971],\n",
      "         [-1.5693, -3.1387],\n",
      "         [-1.5904, -3.1807],\n",
      "         [-1.6116, -3.2232],\n",
      "         [-1.6330, -3.2660],\n",
      "         [-1.6547, -3.3094],\n",
      "         [-1.6766, -3.3531],\n",
      "         [-1.6987, -3.3973],\n",
      "         [-1.7210, -3.4420],\n",
      "         [-1.7435, -3.4871],\n",
      "         [-1.7663, -3.5326],\n",
      "         [-1.7893, -3.5787],\n",
      "         [-1.8126, -3.6252],\n",
      "         [-1.8361, -3.6721],\n",
      "         [-1.8598, -3.7196],\n",
      "         [-1.8838, -3.7675],\n",
      "         [-1.9080, -3.8159],\n",
      "         [-1.9324, -3.8648],\n",
      "         [-1.9571, -3.9142],\n",
      "         [-1.9821, -3.9641],\n",
      "         [-2.0073, -4.0145],\n",
      "         [-2.0327, -4.0655],\n",
      "         [-2.0585, -4.1169],\n",
      "         [-2.0844, -4.1689],\n",
      "         [-2.1107, -4.2214],\n",
      "         [-2.1372, -4.2744],\n",
      "         [-2.1640, -4.3279],\n",
      "         [-2.1910, -4.3820],\n",
      "         [-2.2183, -4.4367]],\n",
      "\n",
      "        [[-0.5000, -1.0000],\n",
      "         [-0.5102, -1.0203],\n",
      "         [-0.5204, -1.0408],\n",
      "         [-0.5308, -1.0615],\n",
      "         [-0.5412, -1.0825],\n",
      "         [-0.5518, -1.1036],\n",
      "         [-0.5625, -1.1250],\n",
      "         [-0.5733, -1.1465],\n",
      "         [-0.5842, -1.1683],\n",
      "         [-0.5952, -1.1903],\n",
      "         [-0.6063, -1.2126],\n",
      "         [-0.6175, -1.2350],\n",
      "         [-0.6289, -1.2577],\n",
      "         [-0.6403, -1.2807],\n",
      "         [-0.6519, -1.3038],\n",
      "         [-0.6636, -1.3272],\n",
      "         [-0.6754, -1.3508],\n",
      "         [-0.6874, -1.3747],\n",
      "         [-0.6994, -1.3988],\n",
      "         [-0.7116, -1.4232],\n",
      "         [-0.7239, -1.4478],\n",
      "         [-0.7363, -1.4727],\n",
      "         [-0.7489, -1.4978],\n",
      "         [-0.7616, -1.5231],\n",
      "         [-0.7744, -1.5488],\n",
      "         [-0.7873, -1.5746],\n",
      "         [-0.8004, -1.6008],\n",
      "         [-0.8136, -1.6272],\n",
      "         [-0.8269, -1.6539],\n",
      "         [-0.8404, -1.6808],\n",
      "         [-0.8540, -1.7080],\n",
      "         [-0.8678, -1.7355],\n",
      "         [-0.8816, -1.7633],\n",
      "         [-0.8957, -1.7913],\n",
      "         [-0.9098, -1.8197],\n",
      "         [-0.9241, -1.8483],\n",
      "         [-0.9386, -1.8772],\n",
      "         [-0.9532, -1.9064],\n",
      "         [-0.9680, -1.9359],\n",
      "         [-0.9829, -1.9657],\n",
      "         [-0.9979, -1.9958],\n",
      "         [-1.0131, -2.0262],\n",
      "         [-1.0285, -2.0569],\n",
      "         [-1.0440, -2.0880],\n",
      "         [-1.0596, -2.1193],\n",
      "         [-1.0755, -2.1510],\n",
      "         [-1.0915, -2.1829],\n",
      "         [-1.1076, -2.2152],\n",
      "         [-1.1239, -2.2479],\n",
      "         [-1.1404, -2.2808],\n",
      "         [-1.1571, -2.3141],\n",
      "         [-1.1739, -2.3478],\n",
      "         [-1.1909, -2.3817],\n",
      "         [-1.2080, -2.4161],\n",
      "         [-1.2254, -2.4507],\n",
      "         [-1.2429, -2.4857],\n",
      "         [-1.2606, -2.5211],\n",
      "         [-1.2784, -2.5569],\n",
      "         [-1.2965, -2.5930],\n",
      "         [-1.3147, -2.6294],\n",
      "         [-1.3331, -2.6663],\n",
      "         [-1.3517, -2.7035],\n",
      "         [-1.3705, -2.7411],\n",
      "         [-1.3895, -2.7791],\n",
      "         [-1.4087, -2.8174],\n",
      "         [-1.4281, -2.8562],\n",
      "         [-1.4477, -2.8953],\n",
      "         [-1.4674, -2.9349],\n",
      "         [-1.4874, -2.9748],\n",
      "         [-1.5076, -3.0152],\n",
      "         [-1.5280, -3.0559],\n",
      "         [-1.5485, -3.0971],\n",
      "         [-1.5693, -3.1387],\n",
      "         [-1.5904, -3.1807],\n",
      "         [-1.6116, -3.2232],\n",
      "         [-1.6330, -3.2660],\n",
      "         [-1.6547, -3.3094],\n",
      "         [-1.6766, -3.3531],\n",
      "         [-1.6987, -3.3973],\n",
      "         [-1.7210, -3.4420],\n",
      "         [-1.7435, -3.4871],\n",
      "         [-1.7663, -3.5326],\n",
      "         [-1.7893, -3.5787],\n",
      "         [-1.8126, -3.6252],\n",
      "         [-1.8361, -3.6721],\n",
      "         [-1.8598, -3.7196],\n",
      "         [-1.8838, -3.7675],\n",
      "         [-1.9080, -3.8159],\n",
      "         [-1.9324, -3.8648],\n",
      "         [-1.9571, -3.9142],\n",
      "         [-1.9821, -3.9641],\n",
      "         [-2.0073, -4.0145],\n",
      "         [-2.0327, -4.0655],\n",
      "         [-2.0585, -4.1169],\n",
      "         [-2.0844, -4.1689],\n",
      "         [-2.1107, -4.2214],\n",
      "         [-2.1372, -4.2744],\n",
      "         [-2.1640, -4.3279],\n",
      "         [-2.1910, -4.3820],\n",
      "         [-2.2183, -4.4367]]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2931/1092302168.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  S_tensor = torch.tensor(S)  # Convert S to a PyTorch tensor\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "# Define problem matrices\n",
    "H = torch.tensor([[1, 0], [0, 1]], dtype=torch.float64)\n",
    "M = torch.tensor([[1, 0], [0, 1]], dtype=torch.float64)\n",
    "D = torch.tensor([[1, 0], [0, 1]], dtype=torch.float64)\n",
    "C = torch.tensor([[1, 0], [0, 1]], dtype=torch.float64)\n",
    "R = torch.tensor([[1, 0], [0, 1]], dtype=torch.float64)\n",
    "T = torch.tensor(1.0, dtype=torch.float64)\n",
    "\n",
    "# Initialize LQRController\n",
    "controller = LQRController(H, M, D, C, R, T)\n",
    "\n",
    "# Define time grid\n",
    "time_grid = torch.linspace(0, T, 100)\n",
    "\n",
    "# Define state values\n",
    "x_values = torch.tensor([[[0.5, 0.5]], [[1.0, 1.0]]], dtype=torch.float64)  # Example state values\n",
    "\n",
    "# Calculate control problem value\n",
    "control_value = controller.optimal_control_value(time_grid, x_values)\n",
    "print(\"Control Problem Value:\")\n",
    "print(control_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control Function:\n",
      "tensor([[[-0.5000, -0.5000],\n",
      "         [-1.0000, -1.0000]],\n",
      "\n",
      "        [[-0.5102, -0.5102],\n",
      "         [-1.0203, -1.0203]],\n",
      "\n",
      "        [[-0.5204, -0.5204],\n",
      "         [-1.0408, -1.0408]],\n",
      "\n",
      "        [[-0.5308, -0.5308],\n",
      "         [-1.0615, -1.0615]],\n",
      "\n",
      "        [[-0.5412, -0.5412],\n",
      "         [-1.0825, -1.0825]],\n",
      "\n",
      "        [[-0.5518, -0.5518],\n",
      "         [-1.1036, -1.1036]],\n",
      "\n",
      "        [[-0.5625, -0.5625],\n",
      "         [-1.1250, -1.1250]],\n",
      "\n",
      "        [[-0.5733, -0.5733],\n",
      "         [-1.1465, -1.1465]],\n",
      "\n",
      "        [[-0.5842, -0.5842],\n",
      "         [-1.1683, -1.1683]],\n",
      "\n",
      "        [[-0.5952, -0.5952],\n",
      "         [-1.1903, -1.1903]],\n",
      "\n",
      "        [[-0.6063, -0.6063],\n",
      "         [-1.2126, -1.2126]],\n",
      "\n",
      "        [[-0.6175, -0.6175],\n",
      "         [-1.2350, -1.2350]],\n",
      "\n",
      "        [[-0.6289, -0.6289],\n",
      "         [-1.2577, -1.2577]],\n",
      "\n",
      "        [[-0.6403, -0.6403],\n",
      "         [-1.2807, -1.2807]],\n",
      "\n",
      "        [[-0.6519, -0.6519],\n",
      "         [-1.3038, -1.3038]],\n",
      "\n",
      "        [[-0.6636, -0.6636],\n",
      "         [-1.3272, -1.3272]],\n",
      "\n",
      "        [[-0.6754, -0.6754],\n",
      "         [-1.3508, -1.3508]],\n",
      "\n",
      "        [[-0.6874, -0.6874],\n",
      "         [-1.3747, -1.3747]],\n",
      "\n",
      "        [[-0.6994, -0.6994],\n",
      "         [-1.3988, -1.3988]],\n",
      "\n",
      "        [[-0.7116, -0.7116],\n",
      "         [-1.4232, -1.4232]],\n",
      "\n",
      "        [[-0.7239, -0.7239],\n",
      "         [-1.4478, -1.4478]],\n",
      "\n",
      "        [[-0.7363, -0.7363],\n",
      "         [-1.4727, -1.4727]],\n",
      "\n",
      "        [[-0.7489, -0.7489],\n",
      "         [-1.4978, -1.4978]],\n",
      "\n",
      "        [[-0.7616, -0.7616],\n",
      "         [-1.5231, -1.5231]],\n",
      "\n",
      "        [[-0.7744, -0.7744],\n",
      "         [-1.5488, -1.5488]],\n",
      "\n",
      "        [[-0.7873, -0.7873],\n",
      "         [-1.5746, -1.5746]],\n",
      "\n",
      "        [[-0.8004, -0.8004],\n",
      "         [-1.6008, -1.6008]],\n",
      "\n",
      "        [[-0.8136, -0.8136],\n",
      "         [-1.6272, -1.6272]],\n",
      "\n",
      "        [[-0.8269, -0.8269],\n",
      "         [-1.6539, -1.6539]],\n",
      "\n",
      "        [[-0.8404, -0.8404],\n",
      "         [-1.6808, -1.6808]],\n",
      "\n",
      "        [[-0.8540, -0.8540],\n",
      "         [-1.7080, -1.7080]],\n",
      "\n",
      "        [[-0.8678, -0.8678],\n",
      "         [-1.7355, -1.7355]],\n",
      "\n",
      "        [[-0.8816, -0.8816],\n",
      "         [-1.7633, -1.7633]],\n",
      "\n",
      "        [[-0.8957, -0.8957],\n",
      "         [-1.7913, -1.7913]],\n",
      "\n",
      "        [[-0.9098, -0.9098],\n",
      "         [-1.8197, -1.8197]],\n",
      "\n",
      "        [[-0.9241, -0.9241],\n",
      "         [-1.8483, -1.8483]],\n",
      "\n",
      "        [[-0.9386, -0.9386],\n",
      "         [-1.8772, -1.8772]],\n",
      "\n",
      "        [[-0.9532, -0.9532],\n",
      "         [-1.9064, -1.9064]],\n",
      "\n",
      "        [[-0.9680, -0.9680],\n",
      "         [-1.9359, -1.9359]],\n",
      "\n",
      "        [[-0.9829, -0.9829],\n",
      "         [-1.9657, -1.9657]],\n",
      "\n",
      "        [[-0.9979, -0.9979],\n",
      "         [-1.9958, -1.9958]],\n",
      "\n",
      "        [[-1.0131, -1.0131],\n",
      "         [-2.0262, -2.0262]],\n",
      "\n",
      "        [[-1.0285, -1.0285],\n",
      "         [-2.0569, -2.0569]],\n",
      "\n",
      "        [[-1.0440, -1.0440],\n",
      "         [-2.0880, -2.0880]],\n",
      "\n",
      "        [[-1.0596, -1.0596],\n",
      "         [-2.1193, -2.1193]],\n",
      "\n",
      "        [[-1.0755, -1.0755],\n",
      "         [-2.1510, -2.1510]],\n",
      "\n",
      "        [[-1.0915, -1.0915],\n",
      "         [-2.1829, -2.1829]],\n",
      "\n",
      "        [[-1.1076, -1.1076],\n",
      "         [-2.2152, -2.2152]],\n",
      "\n",
      "        [[-1.1239, -1.1239],\n",
      "         [-2.2479, -2.2479]],\n",
      "\n",
      "        [[-1.1404, -1.1404],\n",
      "         [-2.2808, -2.2808]],\n",
      "\n",
      "        [[-1.1571, -1.1571],\n",
      "         [-2.3141, -2.3141]],\n",
      "\n",
      "        [[-1.1739, -1.1739],\n",
      "         [-2.3478, -2.3478]],\n",
      "\n",
      "        [[-1.1909, -1.1909],\n",
      "         [-2.3817, -2.3817]],\n",
      "\n",
      "        [[-1.2080, -1.2080],\n",
      "         [-2.4161, -2.4161]],\n",
      "\n",
      "        [[-1.2254, -1.2254],\n",
      "         [-2.4507, -2.4507]],\n",
      "\n",
      "        [[-1.2429, -1.2429],\n",
      "         [-2.4857, -2.4857]],\n",
      "\n",
      "        [[-1.2606, -1.2606],\n",
      "         [-2.5211, -2.5211]],\n",
      "\n",
      "        [[-1.2784, -1.2784],\n",
      "         [-2.5569, -2.5569]],\n",
      "\n",
      "        [[-1.2965, -1.2965],\n",
      "         [-2.5930, -2.5930]],\n",
      "\n",
      "        [[-1.3147, -1.3147],\n",
      "         [-2.6294, -2.6294]],\n",
      "\n",
      "        [[-1.3331, -1.3331],\n",
      "         [-2.6663, -2.6663]],\n",
      "\n",
      "        [[-1.3517, -1.3517],\n",
      "         [-2.7035, -2.7035]],\n",
      "\n",
      "        [[-1.3705, -1.3705],\n",
      "         [-2.7411, -2.7411]],\n",
      "\n",
      "        [[-1.3895, -1.3895],\n",
      "         [-2.7791, -2.7791]],\n",
      "\n",
      "        [[-1.4087, -1.4087],\n",
      "         [-2.8174, -2.8174]],\n",
      "\n",
      "        [[-1.4281, -1.4281],\n",
      "         [-2.8562, -2.8562]],\n",
      "\n",
      "        [[-1.4477, -1.4477],\n",
      "         [-2.8953, -2.8953]],\n",
      "\n",
      "        [[-1.4674, -1.4674],\n",
      "         [-2.9349, -2.9349]],\n",
      "\n",
      "        [[-1.4874, -1.4874],\n",
      "         [-2.9748, -2.9748]],\n",
      "\n",
      "        [[-1.5076, -1.5076],\n",
      "         [-3.0152, -3.0152]],\n",
      "\n",
      "        [[-1.5280, -1.5280],\n",
      "         [-3.0559, -3.0559]],\n",
      "\n",
      "        [[-1.5485, -1.5485],\n",
      "         [-3.0971, -3.0971]],\n",
      "\n",
      "        [[-1.5693, -1.5693],\n",
      "         [-3.1387, -3.1387]],\n",
      "\n",
      "        [[-1.5904, -1.5904],\n",
      "         [-3.1807, -3.1807]],\n",
      "\n",
      "        [[-1.6116, -1.6116],\n",
      "         [-3.2232, -3.2232]],\n",
      "\n",
      "        [[-1.6330, -1.6330],\n",
      "         [-3.2660, -3.2660]],\n",
      "\n",
      "        [[-1.6547, -1.6547],\n",
      "         [-3.3094, -3.3094]],\n",
      "\n",
      "        [[-1.6766, -1.6766],\n",
      "         [-3.3531, -3.3531]],\n",
      "\n",
      "        [[-1.6987, -1.6987],\n",
      "         [-3.3973, -3.3973]],\n",
      "\n",
      "        [[-1.7210, -1.7210],\n",
      "         [-3.4420, -3.4420]],\n",
      "\n",
      "        [[-1.7435, -1.7435],\n",
      "         [-3.4871, -3.4871]],\n",
      "\n",
      "        [[-1.7663, -1.7663],\n",
      "         [-3.5326, -3.5326]],\n",
      "\n",
      "        [[-1.7893, -1.7893],\n",
      "         [-3.5787, -3.5787]],\n",
      "\n",
      "        [[-1.8126, -1.8126],\n",
      "         [-3.6252, -3.6252]],\n",
      "\n",
      "        [[-1.8361, -1.8361],\n",
      "         [-3.6721, -3.6721]],\n",
      "\n",
      "        [[-1.8598, -1.8598],\n",
      "         [-3.7196, -3.7196]],\n",
      "\n",
      "        [[-1.8838, -1.8838],\n",
      "         [-3.7675, -3.7675]],\n",
      "\n",
      "        [[-1.9080, -1.9080],\n",
      "         [-3.8159, -3.8159]],\n",
      "\n",
      "        [[-1.9324, -1.9324],\n",
      "         [-3.8648, -3.8648]],\n",
      "\n",
      "        [[-1.9571, -1.9571],\n",
      "         [-3.9142, -3.9142]],\n",
      "\n",
      "        [[-1.9821, -1.9821],\n",
      "         [-3.9641, -3.9641]],\n",
      "\n",
      "        [[-2.0073, -2.0073],\n",
      "         [-4.0145, -4.0145]],\n",
      "\n",
      "        [[-2.0327, -2.0327],\n",
      "         [-4.0655, -4.0655]],\n",
      "\n",
      "        [[-2.0585, -2.0585],\n",
      "         [-4.1169, -4.1169]],\n",
      "\n",
      "        [[-2.0844, -2.0844],\n",
      "         [-4.1689, -4.1689]],\n",
      "\n",
      "        [[-2.1107, -2.1107],\n",
      "         [-4.2214, -4.2214]],\n",
      "\n",
      "        [[-2.1372, -2.1372],\n",
      "         [-4.2744, -4.2744]],\n",
      "\n",
      "        [[-2.1640, -2.1640],\n",
      "         [-4.3279, -4.3279]],\n",
      "\n",
      "        [[-2.1910, -2.1910],\n",
      "         [-4.3820, -4.3820]],\n",
      "\n",
      "        [[-2.2183, -2.2183],\n",
      "         [-4.4367, -4.4367]]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2931/1092302168.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  S_tensor = torch.tensor(S)  # Convert S to a PyTorch tensor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate control function\n",
    "control_function = controller.optimal_control_function(time_grid, x_values)\n",
    "print(\"\\nControl Function:\")\n",
    "print(control_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_solution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for exercise 2.1\n",
    "class DGM_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(DGM_Layer, self).__init__()\n",
    "        \n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "            \n",
    "\n",
    "        self.gate_Z = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_G = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_R = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_H = self.layer(dim_x+dim_S, dim_S)\n",
    "            \n",
    "    def layer(self, nIn, nOut):\n",
    "        l = nn.Sequential(nn.Linear(nIn, nOut), self.activation)\n",
    "        return l\n",
    "    \n",
    "    def forward(self, x, S):\n",
    "        x_S = torch.cat([x,S],1)\n",
    "        Z = self.gate_Z(x_S)\n",
    "        G = self.gate_G(x_S)\n",
    "        R = self.gate_R(x_S)\n",
    "        \n",
    "        input_gate_H = torch.cat([x, S*R],1)\n",
    "        H = self.gate_H(input_gate_H)\n",
    "        \n",
    "        output = ((1-G))*H + Z*S\n",
    "        return output\n",
    "\n",
    "\n",
    "class Net_DGM(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(Net_DGM, self).__init__()\n",
    "\n",
    "        self.dim = dim_x\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "\n",
    "        self.input_layer = nn.Sequential(nn.Linear(dim_x+1, dim_S), self.activation)\n",
    "\n",
    "        self.DGM1 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM2 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM3 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "\n",
    "        self.output_layer = nn.Linear(dim_S, 1)\n",
    "\n",
    "    def forward(self,t,x):\n",
    "        tx = torch.cat([t,x], 1)\n",
    "        S1 = self.input_layer(tx)\n",
    "        S2 = self.DGM1(tx,S1)\n",
    "        S3 = self.DGM2(tx,S2)\n",
    "        S4 = self.DGM3(tx,S3)\n",
    "        output = self.output_layer(S4)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for exercise 2.2\n",
    "class FFN(nn.Module):\n",
    "\n",
    "    def __init__(self, sizes, activation=nn.ReLU, output_activation=nn.Identity, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.BatchNorm1d(sizes[0]),] if batch_norm else []\n",
    "        for j in range(len(sizes)-1):\n",
    "            layers.append(nn.Linear(sizes[j], sizes[j+1]))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(sizes[j+1], affine=True))\n",
    "            if j<(len(sizes)-2):\n",
    "                layers.append(activation())\n",
    "            else:\n",
    "                layers.append(output_activation())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def freeze(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "sizes = [input_size, 100, 100, output_size]  # Specify sizes of input, hidden, and output layers\n",
    "ffn_model = FFN(sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
